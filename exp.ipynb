{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "words = {}\n",
    "d = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "31 !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class\\\\20_newsgroups'\n",
    "lis = os.listdir(dir)\n",
    "print(lis)\n",
    "punc = string.punctuation\n",
    "punc = punc[0:6] + punc[7:34]\n",
    "print(len(punc),punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 'C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class'\n",
    "os.chdir(sp)\n",
    "file = open(\"C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class\\\\stop.txt\",\"r\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "i=0\n",
    "j=0\n",
    "s=''\n",
    "while(i<len(text)):\n",
    "    if (text[i]!='\\n'):\n",
    "        s = s + text[i]\n",
    "        i = i+1\n",
    "    else :\n",
    "        stopwords.append(s)\n",
    "        s= ''\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.append('a')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords[0] = 'yourselves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(lis)):\n",
    "    files = {}\n",
    "    path = 'C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class\\\\20_newsgroups\\\\' + lis[d]\n",
    "    filelist = os.listdir(path)\n",
    "    os.chdir(path)\n",
    "    words[d] = {}\n",
    "    dd = 0\n",
    "\n",
    "    for filename in filelist:\n",
    "        if not filename in files:\n",
    "            with open(filename, \"r\") as file:\n",
    "                files[filename] = 1\n",
    "                words[d][dd] = {}\n",
    "                line = file.readline()\n",
    "                while(line):\n",
    "                    i=0\n",
    "                    s=''\n",
    "                    text = line.strip()\n",
    "                    f=0\n",
    "                    while(i<len(text)):\n",
    "                        if (not(text[i]==' ')) and (not text[i] in punc):\n",
    "                            s = s + text[i]\n",
    "                            f=1\n",
    "                        else :\n",
    "                            if f==1 and (not s in stopwords):\n",
    "                                if not s in words[d][dd].keys():\n",
    "                                    words[d][dd][s] = 1\n",
    "                                else :\n",
    "                                    words[d][dd][s] = words[d][dd][s] + 1\n",
    "                            s=''\n",
    "                            f=0\n",
    "                        i = i + 1\n",
    "                    if s !='' and (not s in stopwords):\n",
    "                                if not s in words[d][dd].keys():\n",
    "                                    words[d][dd][s] = 1\n",
    "                                else :\n",
    "                                    words[d][dd][s] = words[d][dd][s] + 1\n",
    "                    s= ''\n",
    "                    line = file.readline()\n",
    "                dd = dd + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'alt', 'atheism', '49960', 'moderated', '713', 'news', 'answers', '7054', '126', 'Path', 'crabapple', 'bb3', 'andrew', 'sei', 'cis', 'ohio', 'state', 'magnus', 'acs', 'usenet', 'ins', 'cwru', 'agate', 'spool', 'mu', 'uunet', 'pipex', 'ibmpcug', 'mantis', 'mathew', 'From', 'co', 'uk', 'Newsgroups', 'Subject', 'Alt', 'Atheism', 'FAQ', 'Atheist', 'Resources', 'Summary', 'Books', 'addresses', 'music', 'anything', 'related', 'Keywords', 'books', 'fiction', 'contacts', 'Message', 'ID', '19930329115719', 'Date', 'Mon', '29', 'Mar', '1993', '11', '57', '19', 'GMT', 'Expires', 'Thu', 'Apr', 'Followup', 'To', 'Distribution', 'world', 'Organization', 'Mantis', 'Consultants', 'Cambridge', 'UK', 'Approved', 'request', 'mit', 'Supersedes', '19930301143317', 'Lines', '290', 'Archive', 'name', 'resources', 'archive', 'Last', 'modified', 'December', '1992', 'Version', '1', '0', 'Addresses', 'Organizations', 'USA', 'FREEDOM', 'FROM', 'RELIGION', 'FOUNDATION', 'Darwin', 'fish', 'bumper', 'stickers', 'assorted', 'atheist', 'paraphernalia', 'available', 'Freedom', 'Religion', 'Foundation', 'US', 'Write', 'FFRF', 'P', 'O', 'Box', '750', 'Madison', 'WI', '53701', 'Telephone', '608', '256', '8900', 'EVOLUTION', 'DESIGNS', 'Evolution', 'Designs', 'sell', \"It's\", 'symbol', 'like', 'ones', 'Christians', 'stick', 'cars', 'feet', 'word', 'written', 'inside', 'The', 'deluxe', 'moulded', '3D', 'plastic', '4', '95', 'postpaid', '7119', 'Laurel', 'Canyon', 'North', 'Hollywood', 'CA', '91605', 'People', 'San', 'Francisco', 'Bay', 'area', 'can', 'get', 'Fish', 'Lynn', 'Gold', 'try', 'mailing', 'figmo', 'netcom', 'com', 'For', 'net', 'people', 'go', 'directly', 'price', 'per', 'AMERICAN', 'ATHEIST', 'PRESS', 'AAP', 'publish', 'various', 'critiques', 'Bible', 'lists', 'Biblical', 'contradictions', 'One', 'book', 'Handbook', 'W', 'Ball', 'G', 'Foote', 'American', 'Press', '372', 'pp', 'ISBN', '910309', '26', '2nd', 'edition', '1986', 'absurdities', 'atrocities', 'immoralities', 'contains', 'Contradicts', 'Itself', 'Based', 'King', 'James', 'version', '140195', 'Austin', 'TX', '78714', '0195', '7215', 'Cameron', 'Road', '78752', '2973', '512', '458', '1244', 'Fax', '467', '9525', 'PROMETHEUS', 'BOOKS', 'Sell', 'including', \"Haught's\", 'Holy', 'Horrors', 'see', '700', 'East', 'Amherst', 'Street', 'Buffalo', 'New', 'York', '14215', '716', '837', '2475', 'An', 'alternate', 'address', 'may', 'newer', 'older', 'Prometheus', '59', 'Glenn', 'Drive', 'NY', '14228', '2197', 'AFRICAN', 'AMERICANS', 'FOR', 'HUMANISM', 'organization', 'promoting', 'black', 'secular', 'humanism', 'uncovering', 'history', 'freethought', 'They', 'quarterly', 'newsletter', 'AAH', 'EXAMINER', 'Norm', 'R', 'Allen', 'Jr', 'African', 'Americans', 'Humanism', '664', '14226', 'United', 'Kingdom', 'Rationalist', 'Association', 'National', 'Secular', 'Society', '88', 'Islington', 'High', '702', 'Holloway', 'London', 'N1', '8EW', 'N19', '3NL', '071', '226', '7251', '272', '1266', 'British', 'Humanist', 'South', 'Place', 'Ethical', '14', \"Lamb's\", 'Conduit', 'Passage', 'Conway', 'Hall', 'WC1R', '4RH', 'Red', 'Lion', 'Square', '430', '0908', '4RL', 'fax', '1271', '831', '7723', 'Freethinker', 'monthly', 'magazine', 'founded', '1881', 'Germany', 'IBKA', 'e', 'V', 'Internationaler', 'Bund', 'der', 'Konfessionslosen', 'und', 'Atheisten', 'Postfach', '880', 'D', '1000', 'Berlin', '41', 'journal', 'MIZ', 'Materialien', 'Informationen', 'zur', 'Zeit', 'Politisches', 'Journal', 'Konfessionslosesn', 'Hrsg', 'Vertrieb', 'write', 'IBDK', 'B', 'ucherdienst', '3005', '3000', 'Hannover', '0511', '211216', 'Fiction', 'THOMAS', 'M', 'DISCH', 'Santa', 'Claus', 'Compromise', 'Short', 'story', 'ultimate', 'proof', 'exists', 'All', 'characters', 'events', 'fictitious', 'Any', 'similarity', 'living', 'dead', 'gods', 'uh', 'well', 'WALTER', 'MILLER', 'JR', 'A', 'Canticle', 'Leibowitz', 'gem', 'post', 'atomic', 'doomsday', 'novel', 'monks', 'spent', 'lives', 'copying', 'blueprints', 'Saint', 'filling', 'sheets', 'paper', 'ink', 'leaving', 'white', 'lines', 'letters', 'EDGAR', 'PANGBORN', 'Davy', 'Post', 'set', 'clerical', 'states', 'church', 'example', 'forbids', 'anyone', 'produce', 'describe', 'use', 'substance', 'containing', 'atoms', 'PHILIP', 'K', 'DICK', 'Philip', 'Dick', 'wrote', 'many', 'philosophical', 'thought', 'provoking', 'short', 'stories', 'novels', 'His', 'bizarre', 'times', 'approachable', 'He', 'mainly', 'SF', 'truth', 'religion', 'rather', 'technology', 'Although', 'often', 'believed', 'met', 'sort', 'God', 'remained', 'sceptical', 'Amongst', 'following', 'relevance', 'Galactic', 'Pot', 'Healer', 'fallible', 'alien', 'deity', 'summons', 'group', 'Earth', 'craftsmen', 'women', 'remote', 'planet', 'raise', 'giant', 'cathedral', 'beneath', 'oceans', 'When', 'begins', 'demand', 'faith', 'earthers', 'pot', 'healer', 'Joe', 'Fernwright', 'unable', 'comply', 'polished', 'ironic', 'amusing', 'Maze', 'Death', 'Noteworthy', 'description', 'based', 'VALIS', 'schizophrenic', 'hero', 'searches', 'hidden', 'mysteries', 'Gnostic', 'Christianity', 'reality', 'fired', 'brain', 'pink', 'laser', 'beam', 'unknown', 'possibly', 'divine', 'origin', 'accompanied', 'dogmatic', 'dismissively', 'friend', 'odd', 'Divine', 'Invasion', 'invades', 'making', 'young', 'woman', 'pregnant', 'returns', 'another', 'star', 'system', 'Unfortunately', 'terminally', 'ill', 'must', 'assisted', 'man', 'whose', 'wired', '24', 'hour', 'easy', 'listening', 'MARGARET', 'ATWOOD', \"Handmaid's\", 'Tale', 'premise', 'Congress', 'mysteriously', 'assassinated', 'fundamentalists', 'quickly', 'take', 'charge', 'nation', 'right', 'diary', \"woman's\", 'life', 'tries', 'live', 'new', 'Christian', 'theocracy', \"Women's\", 'property', 'revoked', 'bank', 'accounts', 'closed', 'sinful', 'luxuries', 'outlawed', 'radio', 'used', 'readings', 'Crimes', 'punished', 'retroactively', 'doctors', 'performed', 'legal', 'abortions', 'old', 'hunted', 'hanged', \"Atwood's\", 'writing', 'style', 'difficult', 'first', 'tale', 'grows', 'chilling', 'goes', 'VARIOUS', 'AUTHORS', 'This', 'somewhat', 'dull', 'rambling', 'work', 'criticized', 'However', 'probably', 'worth', 'reading', 'know', 'fuss', 'It', 'different', 'versions', 'make', 'sure', 'one', 'true', 'Non', 'PETER', 'DE', 'ROSA', 'Vicars', 'Christ', 'Bantam', '1988', 'de', 'Rosa', 'seems', 'even', 'Catholic', 'enlighting', 'papal', 'adulteries', 'fallacies', 'etc', 'German', 'translation', 'Gottes', 'erste', 'Diener', 'Die', 'dunkle', 'Seite', 'des', 'Papsttums', 'Droemer', 'Knaur', '1989', 'MICHAEL', 'MARTIN', 'Philosophical', 'Justification', 'Temple', 'University', 'Philadelphia', 'detailed', 'scholarly', 'justification', 'Contains', 'outstanding', 'appendix', 'defining', 'terminology', 'usage', 'necessarily', 'tendentious', 'Argues', 'negative', 'non', 'belief', 'existence', 'god', 's', 'also', 'positive', 'Includes', 'great', 'refutations', 'challenging', 'arguments', 'particular', 'attention', 'paid', 'refuting', 'contempory', 'theists', 'Platinga', 'Swinburne', '541', 'pages', '87722', '642', '3', 'hardcover', 'paperback', 'Case', 'Against', 'comprehensive', 'critique', 'considers', 'best', 'contemporary', 'defences', 'ultimately', 'demonstrates', 'unsupportable', 'incoherent', '273', '767', '5', 'JAMES', 'TURNER', 'Without', 'Creed', 'Johns', 'Hopkins', 'Baltimore', 'MD', 'Subtitled', 'Origins', 'Unbelief', 'America', 'Examines', 'way', 'unbelief', 'whether', 'agnostic', 'atheistic', 'became', 'mainstream', 'alternative', 'view', 'Focusses', 'period', '1770', '1900', 'considering', 'France', 'Britain', 'emphasis', 'particularly', 'England', 'developments', 'Neither', 'religious', 'secularization', 'intellectual', 'fate', 'single', 'idea', '316', '8018', '2494', 'X', '3407', 'GEORGE', 'SELDES', 'Editor', 'thoughts', 'Ballantine', 'dictionary', 'quotations', 'kind', 'concentrating', 'statements', 'writings', 'explicitly', 'implicitly', 'present', \"person's\", 'philosophy', 'obscure', 'suppressed', 'opinions', 'popular', 'observations', 'traces', 'expressed', 'twisted', 'centuries', 'Quite', 'number', 'derived', \"Cardiff's\", 'What', 'Great', 'Men', 'Think', \"Noyes'\", 'Views', '490', '345', '29887', 'RICHARD', 'SWINBURNE', 'Existence', 'Revised', 'Edition', 'Clarendon', 'Paperbacks', 'Oxford', 'second', 'volume', 'trilogy', 'began', 'Coherence', 'Theism', '1977', 'concluded', 'Faith', 'Reason', '1981', 'In', 'attempts', 'construct', 'series', 'inductive', 'rely', 'upon', 'imputation', 'late', '20th', 'century', 'western', 'values', 'aesthetics', 'supposedly', 'simple', 'conceived', 'decisively', 'rejected', \"Mackie's\", 'Miracle', 'revised', 'includes', 'Appendix', 'makes', 'attempt', 'rebut', 'Mackie', 'J', 'L', 'MACKIE', 'posthumous', 'review', 'principal', 'ranges', 'classical', 'positions', 'Descartes', 'Anselm', 'Berkeley', 'Hume', 'et', 'al', 'moral', 'Newman', 'Kant', 'Sidgwick', 'recent', 'restatements', 'theses', 'Plantinga', 'push', 'concept', 'beyond', 'realm', 'rational', 'Kierkegaard', 'Kung', 'Philips', 'replacements', \"Lelie's\", 'axiarchism', 'delight', 'read', 'less', 'formalistic', 'better', \"Martin's\", 'works', 'refreshingly', 'direct', 'compared', 'hand', 'waving', 'HAUGHT', 'Illustrated', 'History', 'Religious', 'Murder', 'Madness', 'Looks', 'persecution', 'ancient', 'day', 'Library', 'Catalog', 'Card', 'Number', '89', '64079', '1990', 'NORM', 'ALLEN', 'Anthology', 'See', 'listing', 'GORDON', 'STEIN', 'Rationalism', 'anthology', 'covering', 'wide', 'range', 'subjects', \"'The\", 'Devil', 'Evil', \"Morality'\", \"Freethought'\", 'Comprehensive', 'bibliography', 'EDMUND', 'COHEN', 'Mind', 'Believer', 'study', 'become', 'effect', 'Net', \"There's\", 'small', 'mail', 'server', 'carries', 'archives', 'articles', 'files', 'information', 'send', 'saying', 'help', 'index', 'will', 'back', 'reply', 'ÿ'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "997\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n",
      "In type 1 - \n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words.keys())):\n",
    "    print(\"In type 1 - \")\n",
    "    print(len(words[i].keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0][1]['pragmatic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words.keys())):\n",
    "    for j in range(len(words[i].keys())):\n",
    "        for s in words[i][j].keys():\n",
    "            if(not s in freq_words.keys()):\n",
    "                freq_words[s] = 1\n",
    "            else:\n",
    "                freq_words[s] = freq_words[s] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258529"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(freq_words, key=freq_words.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258529"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted_list[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'would' in freq_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yourselves',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"let's\",\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours\\tourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'a']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_n = 0\n",
    "col_n = 0\n",
    "num_col = 3000\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(words.keys())):\n",
    "    for j in range(len(words[i].keys())):\n",
    "        x_train.append([])\n",
    "        for k in range(3000):\n",
    "            if sorted_list[k] in words[i][j].keys():\n",
    "                x_train[row_n].append(words[i][j][sorted_list[k]])\n",
    "            else :\n",
    "                x_train[row_n].append(0)\n",
    "        row_n = row_n + 1\n",
    "        y_train.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 3000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train,y_train):\n",
    "    result = {}\n",
    "    classes = set(y_train)\n",
    "    result[\"total_data\"] = len(y_train)\n",
    "    for current_class in classes:\n",
    "        result[current_class] = {}\n",
    "        num_features = x_train.shape[1]\n",
    "        \n",
    "        current_class_rows = (y_train == current_class)\n",
    "        x_train_current = x_train[current_class_rows]\n",
    "        y_train_current = y_train[current_class_rows]\n",
    "        #result[current_class][\"total_count\"] = len(y_train_current)\n",
    "        sum=0\n",
    "        for j in range(1,num_features+1):\n",
    "            result[current_class][sorted_list[j-1]] = x_train_current[:,j-1].sum()\n",
    "            sum = sum + result[current_class][sorted_list[j-1]]\n",
    "        result[current_class][\"total_word_count\"] = sum\n",
    "        result[current_class][\"class_count\"] = len(y_train_current)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probab(dictionary,x,current_class):\n",
    "    output = np.log(dictionary[current_class][\"class_count\"]) - np.log(20000)\n",
    "    num_features = len(x)\n",
    "    for j in range(1,num_features + 1):\n",
    "        if not x[j-1] in sorted_list:\n",
    "            continue\n",
    "        count_with_vlaue_xj = dictionary[current_class][x[j-1]] + 1\n",
    "        count_with_current_class = dictionary[current_class][\"total_word_count\"] + 3000\n",
    "        prob = np.log(count_with_vlaue_xj) - np.log(count_with_current_class)\n",
    "        output = output + prob\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary,x):\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    classes = dictionary.keys()\n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue;\n",
    "        p = probab(dictionary,x,current_class)\n",
    "        if(p > best_p):\n",
    "            best_p = p\n",
    "            best_class = current_class\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary,x_test):\n",
    "    y_pred = []\n",
    "    for x in x_test:\n",
    "        x_class = predictSinglePoint(dictionary,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class\\\\mini_newsgroups'\n",
    "lis = os.listdir(dir)\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "row = 0\n",
    "cl = 0\n",
    "for d in range(len(lis)):\n",
    "    files = {}\n",
    "    path = 'C:\\\\Users\\\\yadav\\\\ML\\\\naive bayes\\\\text-class\\\\mini_newsgroups\\\\' + lis[d]\n",
    "    filelist = os.listdir(path)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for filename in filelist:\n",
    "        x_test.append([])\n",
    "        if not filename in files:\n",
    "            with open(filename, \"r\") as file:\n",
    "                files[filename] = 1\n",
    "                line = file.readline()\n",
    "                while(line):\n",
    "                    i=0\n",
    "                    s=''\n",
    "                    text = line.strip()\n",
    "                    f=0\n",
    "                    while(i<len(text)):\n",
    "                        if (not(text[i]==' ')) and (not text[i] in punc):\n",
    "                            s = s + text[i]\n",
    "                            f=1\n",
    "                        else :\n",
    "                            if f==1 and (not s in stopwords):\n",
    "                                if not s in x_test[row]:\n",
    "                                    #print(s,\"in row\",row)\n",
    "                                    x_test[row].append(s)\n",
    "                            s=''\n",
    "                            f=0\n",
    "                        i = i + 1\n",
    "                    if s !='' and (not s in stopwords):\n",
    "                                if not s in x_test[row]:\n",
    "                                    x_test[row].append(s)\n",
    "                    s= ''\n",
    "                    line = file.readline()\n",
    "            y_test.append(cl)\n",
    "            row = row + 1\n",
    "    cl = cl+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(dictt,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           0       0.74      0.61      0.67       100\n",
      "           1       0.71      0.75      0.73       100\n",
      "           2       1.00      0.01      0.02       100\n",
      "           3       0.63      0.84      0.72       100\n",
      "           4       0.87      0.85      0.86       100\n",
      "           5       0.90      0.74      0.81       100\n",
      "           6       0.84      0.92      0.88       100\n",
      "           7       0.96      0.74      0.84       100\n",
      "           8       0.94      0.91      0.92       100\n",
      "           9       0.89      0.80      0.84       100\n",
      "          10       1.00      0.70      0.82       100\n",
      "          11       1.00      0.65      0.79       100\n",
      "          12       0.82      0.83      0.83       100\n",
      "          13       0.93      0.68      0.79       100\n",
      "          14       0.93      0.67      0.78       100\n",
      "          15       0.98      0.58      0.73       100\n",
      "          16       0.74      0.57      0.64       100\n",
      "          17       0.98      0.52      0.68       100\n",
      "          18       0.80      0.44      0.57       100\n",
      "          19       0.69      0.41      0.52       100\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.83      0.63      0.69      2000\n",
      "weighted avg       0.87      0.66      0.72      2000\n",
      "\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [23 61  0  0  0  0  0  0  1  0  0  0  0  0  2  1  1  1  0  0 10]\n",
      " [16  0 75  0  5  2  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [16  0 17  1 43  6  8  2  0  1  0  0  0  5  0  1  0  0  0  0  0]\n",
      " [13  0  1  0 84  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [14  0  0  0  0 85  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  0  8  0  1  2 74  0  0  0  0  0  0  1  0  1  0  0  0  0  0]\n",
      " [ 5  0  0  0  0  1  0 92  0  0  0  0  0  1  0  0  0  0  0  1  0]\n",
      " [18  0  0  0  0  0  0  4 74  2  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 8  0  0  0  0  0  0  1  0 91  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [18  0  0  0  0  0  0  1  0  0 80  0  0  1  0  0  0  0  0  0  0]\n",
      " [20  0  0  0  0  0  0  0  0  0  9 70  0  0  1  0  0  0  0  0  0]\n",
      " [31  0  1  0  0  0  0  1  1  0  0  0 65  1  0  0  0  0  0  0  0]\n",
      " [16  0  0  0  0  1  0  0  0  0  0  0  0 83  0  0  0  0  0  0  0]\n",
      " [29  0  1  0  0  0  0  0  0  1  0  0  0  1 68  0  0  0  0  0  0]\n",
      " [26  0  2  0  0  0  0  1  1  0  0  0  0  2  1 67  0  0  0  0  0]\n",
      " [41  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0 58  0  0  0  0]\n",
      " [38  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0 57  0  4  0]\n",
      " [39  0  0  0  0  0  0  3  0  1  0  0  0  0  0  0  0  3 52  1  1]\n",
      " [32  0  0  0  0  0  0  1  0  0  0  0  0  0  1  2  0 12  1 44  7]\n",
      " [25 21  0  0  0  0  0  2  0  0  1  0  0  1  0  0  0  4  0  5 41]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-d151ae51ea32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"classes_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0;32m    738\u001b[0m                 self.class_log_prior_)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "Y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test,Y_pred))\n",
    "print(confusion_matrix(y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
